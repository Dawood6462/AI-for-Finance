{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b3e2e5f6b87"
      },
      "source": [
        "# Advanced Feature Engineering in Keras \n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "1. Process temporal feature columns in Keras.\n",
        "2. Use Keras layers to perform feature engineering on geolocation features.\n",
        "3. Create bucketized features, feature crosses and embeddings to combine geospatial and temporal features\n",
        " \n",
        "\n",
        "## Introduction \n",
        "\n",
        "In this notebook, we use Keras to build a taxifare price prediction model and utilize feature engineering to improve the fare amount prediction for NYC taxi cab rides. \n",
        "\n",
        "Each learning objective will correspond to a __#TODO__ in the [student lab notebook](../labs/4_keras_adv_feat_eng-lab.ipynb) -- try to complete that notebook first before reviewing this solution notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e72e40ac9b2"
      },
      "source": [
        "## Set up environment variables and load necessary libraries \n",
        "We will start by importing the necessary libraries for this lab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1adae83b5c95"
      },
      "outputs": [],
      "source": [
        "# Run the chown command to change the ownership of the repository\n",
        "!sudo chown -R jupyter:jupyter /home/jupyter/training-data-analyst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ee17bf60d8b"
      },
      "outputs": [],
      "source": [
        "# You can use any Python source file as a module by executing an import statement in some other Python source file.\n",
        "# The import statement combines two operations; it searches for the named module, then it binds the results of that search\n",
        "# to a name in the local scope.\n",
        "import datetime\n",
        "import logging\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# set TF error log verbosity\n",
        "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
        "\n",
        "print(tf.version.VERSION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c304f719d68e"
      },
      "source": [
        "## Load taxifare dataset\n",
        "\n",
        "The Taxi Fare dataset for this lab is 106,545 rows and has been pre-processed and split for use in this lab.  Note that the dataset is the same as used in the Big Query feature engineering labs.  The fare_amount is the target, the continuous value weâ€™ll train a model to predict.  \n",
        "\n",
        "First, let's download the  .csv data by copying the data from a cloud storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f830961d009"
      },
      "outputs": [],
      "source": [
        "# `os.makedirs()` method will create all unavailable/missing directory in the specified path.\n",
        "if not os.path.isdir(\"../data\"):\n",
        "    os.makedirs(\"../data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76ef4d4ebcfc"
      },
      "outputs": [],
      "source": [
        "# The `gsutil cp` command allows you to copy data between the bucket and current directory.\n",
        "!gsutil cp gs://cloud-training/mlongcp/v3.0_MLonGC/toy_data/taxi-train_toy.csv ../data\n",
        "!gsutil cp gs://cloud-training/mlongcp/v3.0_MLonGC/toy_data/taxi-valid_toy.csv ../data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc8efdc3884f"
      },
      "source": [
        "Let's check that the files were copied correctly and look like we expect them to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3f0b02765dca"
      },
      "outputs": [],
      "source": [
        "# `ls` shows the working directory's contents.\n",
        "# The `l` flag list the all files with permissions and details.\n",
        "!ls -l ../data/*.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8e6bef79e59a"
      },
      "outputs": [],
      "source": [
        "# By default `head` returns the first ten lines of each file.\n",
        "!head ../data/*.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c6b3defbec9"
      },
      "source": [
        "## Create an input pipeline \n",
        "\n",
        "Typically, you will use a two step process to build the pipeline. Step one is to define the columns of data; i.e., which column we're predicting for, and the default values.  Step 2 is to define two functions - a function to define the features and label you want to use and a function to load the training data.  Also, note that pickup_datetime is a string and we will need to handle this in our feature engineered model.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "501e195d6371"
      },
      "outputs": [],
      "source": [
        "CSV_COLUMNS = [\n",
        "    \"fare_amount\",\n",
        "    \"pickup_datetime\",\n",
        "    \"pickup_longitude\",\n",
        "    \"pickup_latitude\",\n",
        "    \"dropoff_longitude\",\n",
        "    \"dropoff_latitude\",\n",
        "    \"passenger_count\",\n",
        "    \"key\",\n",
        "]\n",
        "LABEL_COLUMN = \"fare_amount\"\n",
        "STRING_COLS = [\"pickup_datetime\"]\n",
        "NUMERIC_COLS = [\n",
        "    \"pickup_longitude\",\n",
        "    \"pickup_latitude\",\n",
        "    \"dropoff_longitude\",\n",
        "    \"dropoff_latitude\",\n",
        "    \"passenger_count\",\n",
        "]\n",
        "DEFAULTS = [[0.0], [\"na\"], [0.0], [0.0], [0.0], [0.0], [0.0], [\"na\"]]\n",
        "DAYS = [\"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c53c8246ba9d"
      },
      "outputs": [],
      "source": [
        "# A function to define features and labesl\n",
        "def features_and_labels(row_data):\n",
        "    for unwanted_col in [\"key\"]:\n",
        "        row_data.pop(unwanted_col)\n",
        "    label = row_data.pop(LABEL_COLUMN)\n",
        "    return row_data, label\n",
        "\n",
        "\n",
        "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
        "def load_dataset(pattern, batch_size=1, mode=\"eval\"):\n",
        "    dataset = tf.data.experimental.make_csv_dataset(\n",
        "        pattern, batch_size, CSV_COLUMNS, DEFAULTS\n",
        "    )\n",
        "    dataset = dataset.map(features_and_labels)  # features, label\n",
        "    if mode == \"train\":\n",
        "        dataset = dataset.shuffle(1000).repeat()\n",
        "        # take advantage of multi-threading; 1=AUTOTUNE\n",
        "        dataset = dataset.prefetch(1)\n",
        "    if mode == \"preprocess\":\n",
        "        # For preprocessing steps that require calling the adapt function\n",
        "        dataset = tf.data.experimental.make_csv_dataset(\n",
        "            pattern, batch_size, CSV_COLUMNS, DEFAULTS, num_epochs=1\n",
        "        )\n",
        "        dataset = dataset.map(features_and_labels)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b35b7ace3230"
      },
      "source": [
        "## Create a Baseline DNN Model in Keras\n",
        "\n",
        "Now let's build the Deep Neural Network (DNN) model in Keras using the functional API. Unlike the sequential API, we will need to specify the input and hidden layers.  Note that we are creating a linear regression baseline model with no feature engineering. Recall that a baseline model is a solution to a problem without applying any machine learning techniques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2658675223cc"
      },
      "outputs": [],
      "source": [
        "# Build a simple Keras DNN using its Functional API\n",
        "def rmse(y_true, y_pred):  # Root mean square error\n",
        "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
        "\n",
        "\n",
        "def build_dnn_model():\n",
        "    # input layer\n",
        "    inputs = {\n",
        "        colname: layers.Input(name=colname, shape=(1,), dtype=\"float32\")\n",
        "        for colname in NUMERIC_COLS\n",
        "    }\n",
        "\n",
        "    # Keras layer which receives the input\n",
        "    dnn_inputs = layers.concatenate(inputs.values(), name=\"concatenate_inputs\")\n",
        "\n",
        "    # two hidden layers of [32, 8] just in like the BQML DNN\n",
        "    h1 = layers.Dense(32, activation=\"relu\", name=\"h1\")(dnn_inputs)\n",
        "    h2 = layers.Dense(8, activation=\"relu\", name=\"h2\")(h1)\n",
        "\n",
        "    # final output is a linear activation because this is regression\n",
        "    output = layers.Dense(1, activation=\"linear\", name=\"fare\")(h2)\n",
        "    model = models.Model(inputs, output)\n",
        "\n",
        "    # compile model\n",
        "    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[rmse, \"mse\"])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e06253e67c81"
      },
      "source": [
        "We'll build our DNN model and inspect the model architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cc1601178493"
      },
      "outputs": [],
      "source": [
        "model = build_dnn_model()\n",
        "\n",
        "# We can visualize the DNN using the Keras `plot_model` utility.\n",
        "tf.keras.utils.plot_model(model, \"dnn_model.png\", show_shapes=False, rankdir=\"LR\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fd7c63d2775"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "To train the model, simply call [model.fit()](https://keras.io/models/model/#fit).  Note that we should really use many more NUM_TRAIN_EXAMPLES (i.e. a larger dataset). We shouldn't make assumptions about the quality of the model based on training/evaluating it on a small sample of the full data.\n",
        "\n",
        "We start by setting up the environment variables for training, creating the input pipeline datasets, and then train our baseline DNN model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e083d56a1ab7"
      },
      "outputs": [],
      "source": [
        "TRAIN_BATCH_SIZE = 32\n",
        "NUM_TRAIN_EXAMPLES = 59621 * 5\n",
        "NUM_EVALS = 5\n",
        "NUM_EVAL_EXAMPLES = 14906\n",
        "steps_per_epoch = NUM_TRAIN_EXAMPLES // (TRAIN_BATCH_SIZE * NUM_EVALS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76fcc71ff4d9"
      },
      "outputs": [],
      "source": [
        "# `load_dataset` method is used to load the dataset.\n",
        "trainds = load_dataset(\"../data/taxi-train_toy*\", TRAIN_BATCH_SIZE, \"train\")\n",
        "evalds = load_dataset(\"../data/taxi-valid.csv*\", 1000, \"eval\").take(\n",
        "    NUM_EVAL_EXAMPLES // 1000\n",
        ")\n",
        "pretrainds = load_dataset(\"../data/taxi-train_toy*\", TRAIN_BATCH_SIZE, \"preprocess\")\n",
        "\n",
        "\n",
        "# `Fit` trains the model for a fixed number of epochs\n",
        "history = model.fit(\n",
        "    trainds, validation_data=evalds, epochs=NUM_EVALS, steps_per_epoch=steps_per_epoch\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aade8ac61120"
      },
      "source": [
        "### Visualize the model loss curve\n",
        "\n",
        "Next, we will use matplotlib to draw the model's loss curves for training and validation.  A line plot is also created showing the mean squared error loss over the training epochs for both the train (blue) and test (orange) sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e037fc65203"
      },
      "outputs": [],
      "source": [
        "# A function to define plot_curves.\n",
        "def plot_curves(history, metrics):\n",
        "    nrows = 1\n",
        "    ncols = 2\n",
        "    fig = plt.figure(figsize=(10, 5))\n",
        "\n",
        "    for idx, key in enumerate(metrics):\n",
        "        fig.add_subplot(nrows, ncols, idx + 1)\n",
        "        plt.plot(history.history[key])\n",
        "        plt.plot(history.history[\"val_{}\".format(key)])\n",
        "        plt.title(\"model {}\".format(key))\n",
        "        plt.ylabel(key)\n",
        "        plt.xlabel(\"epoch\")\n",
        "        plt.legend([\"train\", \"validation\"], loc=\"upper left\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "280186554a08"
      },
      "outputs": [],
      "source": [
        "plot_curves(history, [\"loss\", \"mse\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "335ba7ac0477"
      },
      "source": [
        "### Predict with the model locally\n",
        "\n",
        "To predict with Keras, you simply call [model.predict()](https://keras.io/models/model/#predict) and pass in the cab ride you want to predict the fare amount for.  Next we note the fare price at this geolocation and pickup_datetime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65e7ed705ddf"
      },
      "outputs": [],
      "source": [
        "# Use the model to do prediction with `model.predict()`.\n",
        "model.predict(\n",
        "    {\n",
        "        \"pickup_longitude\": tf.convert_to_tensor([-73.982683]),\n",
        "        \"pickup_latitude\": tf.convert_to_tensor([40.742104]),\n",
        "        \"dropoff_longitude\": tf.convert_to_tensor([-73.983766]),\n",
        "        \"dropoff_latitude\": tf.convert_to_tensor([40.755174]),\n",
        "        \"passenger_count\": tf.convert_to_tensor([3.0]),\n",
        "        \"pickup_datetime\": tf.convert_to_tensor(\n",
        "            [\"2010-02-08 09:17:00 UTC\"], dtype=tf.string\n",
        "        ),\n",
        "    },\n",
        "    steps=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b79f4289e54"
      },
      "source": [
        "## Improve Model Performance Using Feature Engineering \n",
        "\n",
        "We now improve our model's performance by creating the following feature engineering types:  Temporal, Categorical, and Geolocation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ea5a0fcf4f9"
      },
      "source": [
        "### Temporal Feature Columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d14a3a06258"
      },
      "source": [
        "We incorporate the temporal feature pickup_datetime.  As noted earlier, pickup_datetime is a string and we will need to handle this within the model.  First, you will include the pickup_datetime as a feature and then you will need to modify the model to handle our string feature. There are many different alternatives for extracting the temporal information from the raw data.  One is to embed python functions and embed them in Lambda layers, another is to use native tensorflow functions to extract this information.  The latter method is implemented in this solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebf8eee6bcc2"
      },
      "outputs": [],
      "source": [
        "# TODO 1a\n",
        "def parse_datetime(s):\n",
        "    if type(s) is not str:\n",
        "        s = s.numpy().decode(\"utf-8\")\n",
        "    return datetime.datetime.strptime(s, \"%Y-%m-%d %H:%M:%S %Z\")\n",
        "\n",
        "\n",
        "# TODO 1b\n",
        "def get_dayofweek(s):\n",
        "    ts = parse_datetime(s)\n",
        "    return DAYS[ts.weekday()]\n",
        "\n",
        "\n",
        "# TODO 1c\n",
        "@tf.function\n",
        "def dayofweek(ts_in):\n",
        "    return tf.map_fn(\n",
        "        lambda s: tf.py_function(get_dayofweek, inp=[s], Tout=tf.string), ts_in\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5303baee35dd"
      },
      "source": [
        "### Geolocation/Coordinate Feature Columns\n",
        "\n",
        "The pick-up/drop-off longitude and latitude data are crucial to predicting the fare amount as fare amounts in NYC taxis are largely determined by the distance traveled. As such, we need to teach the model the Euclidean distance between the pick-up and drop-off points.\n",
        "\n",
        "Recall that latitude and longitude allows us to specify any location on Earth using a set of coordinates. In our training data set, we restricted our data points to only pickups and drop offs within NYC. New York city has an approximate longitude range of -74.05 to -73.75 and a latitude range of 40.63 to 40.85.\n",
        "\n",
        "#### Computing Euclidean distance\n",
        "The dataset contains information regarding the pickup and drop off coordinates. However, there is no information regarding the distance between the pickup and drop off points. Therefore, we create a new feature that calculates the distance between each pair of pickup and drop off points. We can do this using the Euclidean Distance, which is the straight-line distance between any two coordinate points. This distance is computed in the transform function below.  Note that lon1, lon2, etc., below represent tensors, and the math operations are overloaded.  As such, the key to defining new features is to understand how tensors are manipulated symbolically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b404991efb54"
      },
      "outputs": [],
      "source": [
        "def euclidean(params):\n",
        "    lon1, lat1, lon2, lat2 = params\n",
        "    londiff = lon2 - lon1\n",
        "    latdiff = lat2 - lat1\n",
        "    return tf.sqrt(londiff * londiff + latdiff * latdiff)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b0601e14c95"
      },
      "source": [
        "#### Scaling latitude and longitude\n",
        "\n",
        "It is very important for numerical variables to get scaled before they are \"fed\" into the neural network. Here we use min-max scaling (also called normalization) on the geolocation features.  Later in our model, you will see that these values are shifted and rescaled so that they end up ranging from 0 to 1.\n",
        "\n",
        "First, we create a function named 'scale_longitude', where we pass in all the longitudinal values and add 78 to each value.  Note that our scaling longitude ranges from -70 to -78. Thus, the value 78 is the maximum longitudinal value.  The delta or difference between -70 and -78 is 8.  We add 78 to each longitudinal value and then divide by 8 to return a scaled value. The scale longitude function can be included in the transform function below because these mathematical operators are overloaded by tensorflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ec0d57bd13de"
      },
      "outputs": [],
      "source": [
        "def scale_longitude(lon_column):\n",
        "    return (lon_column + 78) / 8.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "400072190931"
      },
      "source": [
        "Next, we create a function named 'scale_latitude', where we pass in all the latitudinal values and subtract 37 from each value.  Note that our scaling longitude ranges from -37 to -45. Thus, the value 37 is the minimal latitudinal value.  The delta or difference between -37 and -45 is 8.  We subtract 37 from each latitudinal value and then divide by 8 to return a scaled value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92aa0751130f"
      },
      "outputs": [],
      "source": [
        "def scale_latitude(lat_column):\n",
        "    return (lat_column - 37) / 8.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f99224351d86"
      },
      "source": [
        "### Normalizing features\n",
        "\n",
        "Often there is some amount of preprocessing that can be done ahead of time to scale the data.  In this case, create_normalizer is learning the distribution of the passenger_count feature so that at training and inference time, it can scale passenger_count using the mean and variance learned at pre-processing time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73aaebc55c66"
      },
      "outputs": [],
      "source": [
        "def create_normalizer(dataset, feature_name):\n",
        "    normalizer = tf.keras.layers.Normalization(\n",
        "        axis=None, name=f\"{feature_name}_normalizer\"\n",
        "    )\n",
        "    feature_ds = dataset.map(lambda X, y: X[feature_name])\n",
        "    normalizer.adapt(feature_ds)\n",
        "    return normalizer\n",
        "\n",
        "\n",
        "passenger_count_normalizer = create_normalizer(pretrainds, \"passenger_count\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "622337c2c3e9"
      },
      "source": [
        "### Putting it all together\n",
        "We now create two new \"geo\" functions for our model.  We create a function called \"euclidean\" to initialize our geolocation parameters.  We then create a function called transform.   The transform function passes our numerical and string column features as inputs to the model, scales geolocation features, then creates the Euclidean distance as a transformed variable with the geolocation features. We then bucketize the latitude and longitude features so that we can represent geographical concepts at a coarser level than longitude and latitude.  We then create feature crosses to represent higher level concepts such as origin and destination.  Building upon this, we create a feature representing the trip itself, which includes the origin and destination.  Since a trip at 3am is different than one at 8am on a Monday in New York, we feed the model information about time.  We then combine the trip and time information in an embedding and feed it to the model so that it can learn efficient representations of these higher level and higher dimensional concepts.  Note that this leads to very sparse features, which require us to have a lot of data so that we can adhere to the 'Rule of five'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "504e365bb10d"
      },
      "outputs": [],
      "source": [
        "def transform(inputs, numeric_cols, string_cols, nbuckets):\n",
        "\n",
        "    # We are going to return both features, which we enriched, and embeddings, which the model will enrich.\n",
        "    transformed = {}\n",
        "    embeddings = {}\n",
        "\n",
        "    # Here, inputs['passenger_count'] is a symbolic placeholder for the tensors that will be\n",
        "    # flowing through this transformation. We are using our normalizer, which we adapted\n",
        "    # before training began.\n",
        "\n",
        "    transformed[\"passenger_count\"] = passenger_count_normalizer(\n",
        "        inputs[\"passenger_count\"]\n",
        "    )\n",
        "\n",
        "    # Note how we could have used the scale_longitude function we defined above.  The two\n",
        "    # statements below are equivalent.\n",
        "    for lon_col in [\"pickup_longitude\", \"dropoff_longitude\"]:\n",
        "        # transformed[lon_col] = scale_longitude(inputs[lon_col])\n",
        "        transformed[lon_col] = (inputs[lon_col] + 78) / 8.0\n",
        "\n",
        "    for lat_col in [\"pickup_latitude\", \"dropoff_latitude\"]:\n",
        "        transformed[lat_col] = (inputs[lat_col] - 37) / 8.0\n",
        "\n",
        "    # Here we compute the euclidean distance between the origin of the trip and the destination\n",
        "    position_difference = tf.square(\n",
        "        inputs[\"dropoff_longitude\"] - inputs[\"pickup_longitude\"]\n",
        "    )\n",
        "    position_difference += tf.square(\n",
        "        inputs[\"dropoff_latitude\"] - inputs[\"pickup_latitude\"]\n",
        "    )\n",
        "    transformed[\"euclidean\"] = tf.sqrt(position_difference)\n",
        "\n",
        "    # We are taking a continuous variable and making it discrete (categorical) with the intention\n",
        "    # of \"crossing\" it with another feature\n",
        "    lat_lon_buckets = [bin_edge / nbuckets for bin_edge in range(1, nbuckets)]\n",
        "    discretization_layer = tf.keras.layers.Discretization(\n",
        "        bin_boundaries=lat_lon_buckets, output_mode=\"int\"\n",
        "    )\n",
        "\n",
        "    # Here discretization_layer is putting the data in ten buckets.  Note that it did not need to\n",
        "    # be adapted, since we are defining the bucket boundaries in this case.  You can\n",
        "    # have it learn the bucket boundaries if you so choose. Note that we are not yet one-hot\n",
        "    # encoding this feature (output_mode='int') as we will compute a hash of this feature, which\n",
        "    # we can only do with integers and strings\n",
        "\n",
        "    bucketed_pickup_longitude_intermediary = discretization_layer(\n",
        "        transformed[\"pickup_longitude\"]\n",
        "    )\n",
        "    bucketed_pickup_latitude_intermediary = discretization_layer(\n",
        "        transformed[\"pickup_latitude\"]\n",
        "    )\n",
        "    bucketed_dropoff_longitude_intermediary = discretization_layer(\n",
        "        transformed[\"dropoff_longitude\"]\n",
        "    )\n",
        "    bucketed_dropoff_latitude_intermediary = discretization_layer(\n",
        "        transformed[\"dropoff_latitude\"]\n",
        "    )\n",
        "\n",
        "    # We are storing float versions of this in our dictionary, which will be returned by our function and\n",
        "    # connected via functional api to the model\n",
        "\n",
        "    transformed[\"bucketed_pickup_longitude\"] = tf.cast(\n",
        "        bucketed_pickup_longitude_intermediary, tf.float32\n",
        "    )\n",
        "    transformed[\"bucketed_pickup_latitude\"] = tf.cast(\n",
        "        bucketed_pickup_latitude_intermediary, tf.float32\n",
        "    )\n",
        "    transformed[\"bucketed_dropoff_longitude\"] = tf.cast(\n",
        "        bucketed_dropoff_longitude_intermediary, tf.float32\n",
        "    )\n",
        "    transformed[\"bucketed_dropoff_latitude\"] = tf.cast(\n",
        "        bucketed_dropoff_latitude_intermediary, tf.float32\n",
        "    )\n",
        "\n",
        "    # Below we are computing higher level features that represent locations.  Note how we now\n",
        "    # use one hot encoding.  This is because this particular feature will not contribute to any\n",
        "    # future hashing operations.  We create another version of this feature below which will\n",
        "    # contribute to even higher-level representations.\n",
        "\n",
        "    hash_pickup_crossing_layer = (\n",
        "        tf.keras.layers.experimental.preprocessing.HashedCrossing(\n",
        "            output_mode=\"one_hot\",\n",
        "            num_bins=nbuckets**2,\n",
        "            name=\"hash_pickup_crossing_layer\",\n",
        "        )\n",
        "    )\n",
        "    transformed[\"pickup_location\"] = hash_pickup_crossing_layer(\n",
        "        (bucketed_pickup_longitude_intermediary, bucketed_pickup_latitude_intermediary)\n",
        "    )\n",
        "    hash_dropoff_crossing_layer = (\n",
        "        tf.keras.layers.experimental.preprocessing.HashedCrossing(\n",
        "            output_mode=\"one_hot\",\n",
        "            num_bins=nbuckets**2,\n",
        "            name=\"hash_dropoff_crossing_layer\",\n",
        "        )\n",
        "    )\n",
        "    transformed[\"dropoff_location\"] = hash_dropoff_crossing_layer(\n",
        "        (\n",
        "            bucketed_dropoff_longitude_intermediary,\n",
        "            bucketed_dropoff_latitude_intermediary,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # We compute int representations of origin and destination\n",
        "    hash_pickup_crossing_layer_intermediary = (\n",
        "        tf.keras.layers.experimental.preprocessing.HashedCrossing(\n",
        "            output_mode=\"int\",\n",
        "            num_bins=nbuckets**2,\n",
        "            name=\"hash_pickup_crossing_intermediary\",\n",
        "        )\n",
        "    )\n",
        "    hashed_pickup_intermediary = hash_pickup_crossing_layer_intermediary(\n",
        "        (bucketed_pickup_longitude_intermediary, bucketed_pickup_latitude_intermediary)\n",
        "    )\n",
        "    hash_dropoff_crossing_layer_intermediary = (\n",
        "        tf.keras.layers.experimental.preprocessing.HashedCrossing(\n",
        "            output_mode=\"int\",\n",
        "            num_bins=nbuckets**2,\n",
        "            name=\"hash_dropoff_crossing_intermediary\",\n",
        "        )\n",
        "    )\n",
        "    hashed_dropoff_intermediary = hash_dropoff_crossing_layer_intermediary(\n",
        "        (\n",
        "            bucketed_dropoff_longitude_intermediary,\n",
        "            bucketed_dropoff_latitude_intermediary,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # We cross origin and destination to dervice a feature that represents the trip.  Remember\n",
        "    # how we partitioned the city into a 10x10 grid? Theoretically, you can have a 100*100\n",
        "    # trip combinations.  It is likely that not all of those are possible in real-life, therefore\n",
        "    # we use only a 1000 buckets below instead of 10,000.\n",
        "    hash_trip_crossing_layer = (\n",
        "        tf.keras.layers.experimental.preprocessing.HashedCrossing(\n",
        "            output_mode=\"one_hot\",\n",
        "            num_bins=nbuckets**3,\n",
        "            name=\"hash_trip_crossing_layer\",\n",
        "        )\n",
        "    )\n",
        "    transformed[\"hashed_trip\"] = hash_trip_crossing_layer(\n",
        "        (hashed_pickup_intermediary, hashed_dropoff_intermediary)\n",
        "    )\n",
        "\n",
        "    # Here we create an embedding, which our model will use to bring trips with similar characteristics\n",
        "    # together.\n",
        "    trip_locations_embedding_layer = tf.keras.layers.Embedding(\n",
        "        input_dim=nbuckets**3,\n",
        "        output_dim=int(nbuckets**1.5),\n",
        "        name=\"trip_locations_embedding_layer\",\n",
        "    )\n",
        "    embeddings[\"trip_locations_embedding\"] = trip_locations_embedding_layer(\n",
        "        transformed[\"hashed_trip\"]\n",
        "    )\n",
        "\n",
        "    # Now that we have dealt with space, let's give some time to temporal features.  We use a tensorflow addon function\n",
        "    # to get the number of seconds since the beginning of the 70s and compute other temporal features.\n",
        "\n",
        "    seconds_since_1970 = tfa.text.parse_time(\n",
        "        inputs[\"pickup_datetime\"], \"%Y-%m-%d %H:%M:%S %Z\", output_unit=\"SECOND\"\n",
        "    )\n",
        "    seconds_since_1970 = tf.cast(seconds_since_1970, tf.float32)\n",
        "    hours_since_1970 = seconds_since_1970 / 3600.0\n",
        "    hours_since_1970 = tf.floor(hours_since_1970)\n",
        "    hour_of_day_intermediary = hours_since_1970 % 24\n",
        "\n",
        "    # Feeding our model hour of day allows it to spot cyclical patterns over the course of the day.\n",
        "    transformed[\"hour_of_day\"] = hour_of_day_intermediary\n",
        "    hour_of_day_intermediary = tf.cast(hour_of_day_intermediary, tf.int32)\n",
        "    days_since_1970 = seconds_since_1970 / (3600 * 24)\n",
        "    days_since_1970 = tf.floor(days_since_1970)\n",
        "\n",
        "    # January 1st 1970 was a Thursday, so we make it so that when it is\n",
        "    # zero days since Jan 1st, 1970, we return a 4.\n",
        "    day_of_week_intermediary = (days_since_1970 + 4) % 7\n",
        "    transformed[\"day_of_week\"] = day_of_week_intermediary\n",
        "    day_of_week_intermediary = tf.cast(day_of_week_intermediary, tf.int32)\n",
        "    hashed_crossing_layer = tf.keras.layers.experimental.preprocessing.HashedCrossing(\n",
        "        num_bins=24 * 7, output_mode=\"one_hot\"\n",
        "    )\n",
        "    hashed_crossing_layer_intermediary = (\n",
        "        tf.keras.layers.experimental.preprocessing.HashedCrossing(\n",
        "            num_bins=24 * 7, output_mode=\"int\", name=\"hashed_hour_of_day_of_week_layer\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Here we send the model a signal about what hour of which day it is.  This is why we created\n",
        "    # 24*7 buckets above\n",
        "    transformed[\"hour_of_day_of_week\"] = hashed_crossing_layer(\n",
        "        (hour_of_day_intermediary, day_of_week_intermediary)\n",
        "    )\n",
        "    hour_of_day_of_week_intermediary = hashed_crossing_layer_intermediary(\n",
        "        (hour_of_day_intermediary, day_of_week_intermediary)\n",
        "    )\n",
        "\n",
        "    # Now it is time to combine our geographical features with our temporal ones.  We create an\n",
        "    # intermediary representation of our trip as an int, so that we can cross it with the temporal\n",
        "    # feature that represents the hour and day of week together.\n",
        "    hash_trip_crossing_layer_intermediary = (\n",
        "        tf.keras.layers.experimental.preprocessing.HashedCrossing(\n",
        "            output_mode=\"int\", num_bins=nbuckets**3\n",
        "        )\n",
        "    )\n",
        "    hashed_trip_intermediary = hash_trip_crossing_layer_intermediary(\n",
        "        (hashed_pickup_intermediary, hashed_dropoff_intermediary)\n",
        "    )\n",
        "\n",
        "    hash_trip_and_time_layer = (\n",
        "        tf.keras.layers.experimental.preprocessing.HashedCrossing(\n",
        "            output_mode=\"one_hot\",\n",
        "            num_bins=(nbuckets**3) * 4,\n",
        "            name=\"hash_trip_and_time_layer\",\n",
        "        )\n",
        "    )\n",
        "    transformed[\"hashed_trip_and_time\"] = hash_trip_and_time_layer(\n",
        "        (hashed_trip_intermediary, hour_of_day_of_week_intermediary)\n",
        "    )\n",
        "    trip_embedding_layer = tf.keras.layers.Embedding(\n",
        "        input_dim=(nbuckets**3) * 4,\n",
        "        output_dim=int(nbuckets**1.5),\n",
        "        name=\"trip_embedding_layer\",\n",
        "    )\n",
        "\n",
        "    # We create an embedding that asks the model to figure out good representations of trips taking both time\n",
        "    # and space into account\n",
        "    embeddings[\"trip_embedding\"] = trip_embedding_layer(\n",
        "        transformed[\"hashed_trip_and_time\"]\n",
        "    )\n",
        "\n",
        "    return transformed, embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82f184b87bf1"
      },
      "source": [
        "Next, we'll create our DNN model now with the engineered features. We'll set `NBUCKETS = 10` to specify 10 buckets when bucketizing the latitude and longitude."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "697b7257c55d"
      },
      "outputs": [],
      "source": [
        "NBUCKETS = 10\n",
        "\n",
        "\n",
        "def build_dnn_model():\n",
        "    # input layer is all float except for pickup_datetime which is a string\n",
        "    inputs = {\n",
        "        colname: layers.Input(name=colname, shape=(1,), dtype=\"float32\")\n",
        "        for colname in NUMERIC_COLS\n",
        "    }\n",
        "    inputs.update(\n",
        "        {\n",
        "            colname: tf.keras.layers.Input(name=colname, shape=(1,), dtype=\"string\")\n",
        "            for colname in STRING_COLS\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # transforms\n",
        "    transformed, embeddings = transform(\n",
        "        inputs, numeric_cols=NUMERIC_COLS, string_cols=STRING_COLS, nbuckets=NBUCKETS\n",
        "    )\n",
        "\n",
        "    dnn_tabular_inputs = tf.keras.layers.Concatenate()(transformed.values())\n",
        "    trip_locations_embedding = embeddings[\"trip_locations_embedding\"]\n",
        "    trip_embedding = embeddings[\"trip_embedding\"]\n",
        "\n",
        "    # two hidden layers of [32, 8] just in like the BQML DNN\n",
        "    # Our model now takes two different types of inputs: features which\n",
        "    # we have engineered (transformed dictionary) and representations\n",
        "    # we want the model to engineer (embeddings).  We use different\n",
        "    # activations for the embeddings as those are the ones supported for\n",
        "    # GPU acceleration.\n",
        "\n",
        "    ht1 = layers.Dense(32, activation=\"relu\", name=\"ht1\")(dnn_tabular_inputs)\n",
        "    ht2 = layers.Dense(8, activation=\"relu\", name=\"ht2\")(ht1)\n",
        "    et1 = layers.LSTM(32, activation=\"tanh\", name=\"et1\")(trip_locations_embedding)\n",
        "    et2 = layers.LSTM(32, activation=\"tanh\", name=\"et2\")(trip_embedding)\n",
        "    merge_layer = layers.concatenate([ht2, et1, et2])\n",
        "    ht3 = layers.Dense(16)(merge_layer)\n",
        "\n",
        "    # final output is a linear activation because this is regression\n",
        "    output = layers.Dense(1, activation=\"linear\", name=\"fare\")(ht3)\n",
        "    model = tf.keras.Model(inputs, output)\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[rmse, \"mse\"])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "947bfbb4dce7"
      },
      "outputs": [],
      "source": [
        "model = build_dnn_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "170b1becacda"
      },
      "source": [
        "Let's see how our model architecture has changed now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8e6b7a5bb4f7"
      },
      "outputs": [],
      "source": [
        "# We can visualize the DNN using the Keras `plot_model` utility.\n",
        "# The plot below represents all of the tensor operations that go into\n",
        "# making a prediction, beginning with the raw input features, preprocessing, feature engineering\n",
        "# and then the prediction itself.\n",
        "tf.keras.utils.plot_model(\n",
        "    model, \"dnn_model_engineered.png\", show_shapes=False, rankdir=\"LR\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ef0db7e499bb"
      },
      "outputs": [],
      "source": [
        "# `load_dataset` method is used to load the dataset.\n",
        "trainds = load_dataset(\"../data/taxi-train_toy*\", TRAIN_BATCH_SIZE, \"train\")\n",
        "evalds = load_dataset(\"../data/taxi-valid.csv*\", 1000, \"eval\").take(\n",
        "    NUM_EVAL_EXAMPLES // 1000\n",
        ")\n",
        "\n",
        "# `Fit` trains the model for a fixed number of epochs.  Note that because we have\n",
        "# created a lot of sparsity, the performance of the model is heavily dependent on\n",
        "# the amount of data that we have for training.  As such, this model overfits on\n",
        "# a small amount of data, as it is complex enough to be able to memorize patterns\n",
        "# over a small amount of data.\n",
        "\n",
        "history = model.fit(\n",
        "    trainds,\n",
        "    validation_data=evalds,\n",
        "    epochs=NUM_EVALS,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a8a84b7b5bd"
      },
      "source": [
        "As before, let's visualize the DNN model layers. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0a1ac34dfa26"
      },
      "outputs": [],
      "source": [
        "plot_curves(history, [\"loss\", \"mse\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c63eae8e7ec5"
      },
      "source": [
        "Let's a prediction with this new model with engineered features on the example we had above. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aab5e62483cb"
      },
      "outputs": [],
      "source": [
        "# Use the model to do prediction with `model.predict()`.\n",
        "# Since our entire pipeline is integrated into the model, the path\n",
        "# that this prediction takes is the same as the training process.\n",
        "# Note that the data format is also the same: a dictionary of tensors!\n",
        "\n",
        "model.predict(\n",
        "    {\n",
        "        \"pickup_longitude\": tf.convert_to_tensor([-73.982683]),\n",
        "        \"pickup_latitude\": tf.convert_to_tensor([40.742104]),\n",
        "        \"dropoff_longitude\": tf.convert_to_tensor([-73.983766]),\n",
        "        \"dropoff_latitude\": tf.convert_to_tensor([40.755174]),\n",
        "        \"passenger_count\": tf.convert_to_tensor([3.0]),\n",
        "        \"pickup_datetime\": tf.convert_to_tensor(\n",
        "            [\"2010-02-08 09:17:00 UTC\"], dtype=tf.string\n",
        "        ),\n",
        "    },\n",
        "    steps=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c997cc0f355"
      },
      "source": [
        "Below we summarize our training results comparing our baseline model with our model with engineered features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4e0f118a456"
      },
      "source": [
        "| Model              | Taxi Fare | Description                               |\n",
        "|--------------------|-----------|-------------------------------------------|\n",
        "| Baseline           | 12.29     | Baseline model - no feature engineering |\n",
        "| Feature Engineered | 07.28      | Feature Engineered Model                |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00ca3a9686c0"
      },
      "source": [
        "Copyright 2021 Google Inc.\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "http://www.apache.org/licenses/LICENSE-2.0\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "4_keras_adv_feat_eng.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
