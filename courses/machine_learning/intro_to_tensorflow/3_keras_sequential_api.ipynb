{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing the Keras Sequential API\n",
    "\n",
    "**Learning Objectives**\n",
    "  - Build a DNN model using the Keras Sequential API\n",
    "  - Learn how to use feature columns in a Keras model\n",
    "  - Learn how to save/load, and deploy a Keras model on GCP\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The [Keras sequential API](https://keras.io/models/sequential/) allows you to create Tensorflow models layer-by-layer. This is useful for building most kinds of machine learnig models but it does not allow you to create models that share layers, re-use layers or have multiple inputs or outputs. In this lab, we'll see how to build a simple deep neural network model using the keras sequential api and feature columns. Once we have trained our model, we will deploy it using AI Platform and see how to call our model for online prediciton.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Ensure that we have the latest version of Tensorflow installed.\n",
    "!pip3 freeze | grep tf-nightly-2.0-preview || pip3 install tf-nightly-2.0-preview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by importing the necessary libraries for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, os, datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow import keras\n",
    "\n",
    "import shutil, os, datetime\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load raw data \n",
    "\n",
    "We will start with the CSV files that we wrote out in the [first notebook](../01_explore/taxifare.iypnb) of this sequence. Just so you don't have to run the notebook, we saved a copy in ../data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l ../data/*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head ../data/taxi*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use tf.data to read the CSV files\n",
    "\n",
    "We wrote these functions for reading data in the [third notebook](../03_tfdata/input_pipeline.ipynb) of this sequence. We set up the column names from our csv files, denote the label column and specify the default values for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_COLUMNS  = ['fare_amount',  'pickup_datetime',\n",
    "                'pickup_longitude', 'pickup_latitude', \n",
    "                'dropoff_longitude', 'dropoff_latitude', \n",
    "                'passenger_count', 'key']\n",
    "LABEL_COLUMN = 'fare_amount'\n",
    "DEFAULTS     = [[0.0],['na'],[0.0],[0.0],[0.0],[0.0],[0.0],['na']]\n",
    "\n",
    "def features_and_labels(row_data):\n",
    "    for unwanted_col in ['pickup_datetime', 'key']:\n",
    "        row_data.pop(unwanted_col)\n",
    "    label = row_data.pop(LABEL_COLUMN)\n",
    "    return row_data, label  # features, label\n",
    "\n",
    "# load the training data\n",
    "def load_dataset(pattern, batch_size=1, mode=tf.estimator.ModeKeys.EVAL):\n",
    "    dataset = (tf.data.experimental.make_csv_dataset(pattern, batch_size, CSV_COLUMNS, DEFAULTS)\n",
    "             .map(features_and_labels) # features, label\n",
    "             .cache())\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        dataset = dataset.shuffle(buffer_size=1000).repeat()\n",
    "    dataset = dataset.prefetch(1) # take advantage of multi-threading; 1=AUTOTUNE\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a simple keras DNN model\n",
    "\n",
    "We will use feature columns to connect our raw data to our keras DNN model. Feature columns make it easy to perform common type of feature engineering on your raw data. For example you can one-hot encode categorical data, create feature crosses, embeddings and more. We'll cover these later in the course, but if you want to a sneak peak browse the official TensorFlow [feature columns guide](https://www.tensorflow.org/guide/feature_columns).\n",
    "\n",
    "In our case we won't do any feature engineering. However we still need to create a list of feature columns to specify the numeric values which will be passed on to our model. To do this we use `tf.feature_column.numeric_column()`\n",
    "\n",
    "We use a python dictionary comprehension to create the feature columns for our model, which is just an elegant alternative to a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_COLS = ['pickup_longitude', 'pickup_latitude', \n",
    "              'dropoff_longitude', 'dropoff_latitude', \n",
    "              'passenger_count']\n",
    "\n",
    "# input layer of feature columns\n",
    "feature_columns = {\n",
    "    colname : tf.feature_column.numeric_column(colname)\n",
    "       for colname in INPUT_COLS\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the DNN model. The Sequential model is a linear stack of layers and when building a model using the Sequential API, you configure each layer of the model in turn. Once all the layers have been added, you compile the model. \n",
    "\n",
    "Before training a model, you must configure the learning process, which is done using the compile method. The compile method takes three arguments:\n",
    "\n",
    "* An optimizer. This could be the string identifier of an existing optimizer (such as `rmsprop` or `adagrad`), or an instance of the [Optimizer class](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/optimizers).\n",
    "* A loss function. This is the objective that the model will try to minimize. It can be the string identifier of an existing loss function from the [Losses class](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/losses) (such as categorical_crossentropy or mse), or it can be a custom objective function.\n",
    "* A list of metrics. For any machine learning problem you will want a set of metrics to evaluate your model. A metric could be the string identifier of an existing metric or a custom metric function.\n",
    "\n",
    "We will add an additional custom metric called `rmse` to our list of metrics which will return the root mean square error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a keras DNN model using Sequential API\n",
    "model = keras.models.Sequential()    \n",
    "model.add(keras.layers.DenseFeatures(feature_columns=feature_columns.values()))\n",
    "model.add(keras.layers.Dense(units=32, activation=\"relu\", name=\"h1\"))\n",
    "model.add(keras.layers.Dense(units=8, activation=\"relu\", name=\"h2\"))\n",
    "model.add(keras.layers.Dense(units=1, activation=\"linear\", name=\"output\"))\n",
    "\n",
    "# Create a custom evalution metric\n",
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true))) \n",
    "\n",
    "# Compile the keras model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=[rmse, 'mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "To train our model, we typically use the `fit()` function. \n",
    "\n",
    "First, we'll set up some training variables like the batch size, the number of training examples, number of evaluations and number of evaluation examples. We'll choose a large number of training examples, since the training dataset repeats (similarly for the eval examples). Next, we create the training and evaluation dataset using the `load_dataset` function we wrote above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 32\n",
    "NUM_TRAIN_EXAMPLES = 10000 * 5 # training dataset repeats, so it will wrap around\n",
    "NUM_EVALS = 5  # how many times to evaluate\n",
    "NUM_EVAL_EXAMPLES = 10000 # enough to get a reasonable sample, but not so much that it slows down\n",
    "\n",
    "trainds = load_dataset(pattern='../data/taxi-train*', \n",
    "                       batch_size=TRAIN_BATCH_SIZE,\n",
    "                       mode=tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "evalds = load_dataset(pattern='../data/taxi-valid*',\n",
    "                      batch_size=1000,\n",
    "                      mode=tf.estimator.ModeKeys.EVAL).take(NUM_EVAL_EXAMPLES//1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various arguments you can set when calling the [.fit() function](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit)\n",
    ". Here we will specify the training data, the evaluation data, the number of epochs for training and a parameter called `steps_per_epoch`. \n",
    "\n",
    "The `steps_per_epoch` parameter is used to mark the end of training for a single epoch. Since the data generator function continuously feeds training samples into the .fit() method, it is not possible to know the end of an epoch unless the parameter steps_per_epoch is specified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "steps_per_epoch = NUM_TRAIN_EXAMPLES // (TRAIN_BATCH_SIZE * NUM_EVALS)\n",
    "\n",
    "history = model.fit(x=trainds,\n",
    "                    validation_data=evalds,\n",
    "                    epochs=NUM_EVALS,\n",
    "                    steps_per_epoch=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High-level model evaluation\n",
    "\n",
    "Once we've run data through the model, we can call `.summary()` on the model to get a high-level summary of our network. We can use the keras utilities to plot a diagram of our model architecture. And, we can plot the training and evaluation curves for the metrics we computed above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, 'dnn_model.png', show_shapes=True, rankdir='LR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "nrows = 1\n",
    "ncols = 2\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "for idx, key in enumerate(['loss', 'rmse']):\n",
    "    ax = fig.add_subplot(nrows, ncols, idx+1)\n",
    "    plt.plot(history.history[key])\n",
    "    plt.plot(history.history['val_{}'.format(key)])\n",
    "    plt.title('model {}'.format(key))\n",
    "    plt.ylabel(key)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making predictions with our model\n",
    "\n",
    "To make predictions with our trained model, we can call the [predict method](https://www.tensorflow.org/api_docs/python/tf/keras/Model#predict), passing to it a dictionary of values. The `steps` parameter determines the total number of steps before declaring the prediction round finished. Here since we have just one example, we set `steps=1` (setting `steps=None` would also work). Note, however, that if x is a `tf.data` dataset or a dataset iterator, and steps is set to None, predict will run until the input dataset is exhausted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(x={'pickup_longitude': tf.convert_to_tensor([-73.982683]),\n",
    "                 'pickup_latitude': tf.convert_to_tensor([40.742104]),\n",
    "                 'dropoff_longitude': tf.convert_to_tensor([-73.983766]),\n",
    "                 'dropoff_latitude': tf.convert_to_tensor([40.755174]),\n",
    "                 'passenger_count': tf.convert_to_tensor([3.0])}, \n",
    "              steps=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export and deploy our model\n",
    "\n",
    "Of course, making individual predictions is not realistic, because we can't expect client code to have a model object in memory. For others to use our trained model, we'll have to export our model to a file, and expect client code to instantiate the model from that exported file. \n",
    "\n",
    "We'll export the model to a TensorFlow SavedModel format. Once we have a model in this format, we have lots of ways to \"serve\" the model, from a web application, from JavaScript, from mobile applications, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"./export/savedmodel\"\n",
    "shutil.rmtree(OUTPUT_DIR, ignore_errors=True)\n",
    "EXPORT_PATH = os.path.join(OUTPUT_DIR, datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "tf.saved_model.save(model, EXPORT_PATH) # with default serving function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --tag_set serve --signature_def serving_default --dir {EXPORT_PATH}\n",
    "!find {EXPORT_PATH}\n",
    "os.environ['EXPORT_PATH'] = EXPORT_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy our model to AI Platform\n",
    "\n",
    "Finally, we will deploy our trained model to AI Platform and see how we can make online predicitons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "PROJECT=munn-sandbox\n",
    "BUCKET=${PROJECT}\n",
    "REGION=us-east1\n",
    "MODEL_NAME=taxifare\n",
    "VERSION_NAME=dnn\n",
    "\n",
    "if [[ $(gcloud ai-platform models list --format='value(name)' | grep $MODEL_NAME) ]]; then\n",
    "    echo \"$MODEL_NAME already exists\"\n",
    "else\n",
    "    # create model\n",
    "    echo \"Creating $MODEL_NAME\"\n",
    "    gcloud ai-platform models create --regions=$REGION $MODEL_NAME\n",
    "fi\n",
    "\n",
    "if [[ $(gcloud ai-platform versions list --model $MODEL_NAME --format='value(name)' | grep $VERSION_NAME) ]]; then\n",
    "    echo \"Deleting already existing $MODEL_NAME:$VERSION_NAME ... \"\n",
    "    gcloud ai-platform versions delete --model=$MODEL_NAME $VERSION_NAME\n",
    "    echo \"Please run this cell again if you don't see a Creating message ... \"\n",
    "    sleep 2\n",
    "fi\n",
    "\n",
    "# create model\n",
    "echo \"Creating $MODEL_NAME:$VERSION_NAME\"\n",
    "gcloud ai-platform versions create --model=$MODEL_NAME $VERSION_NAME --async \\\n",
    "       --framework=tensorflow --python-version=3.5 --runtime-version=1.14 \\\n",
    "       --origin=$EXPORT_PATH --staging-bucket=gs://$BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile input.json\n",
    "{\"pickup_longitude\": -73.982683, \"pickup_latitude\": 40.742104,\"dropoff_longitude\": -73.983766,\"dropoff_latitude\": 40.755174,\"passenger_count\": 3.0}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai-platform predict --model taxifare --json-instances input.json --version dnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2019 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
